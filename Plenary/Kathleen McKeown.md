# Rewriting the Past: Assessing the Field through the Lens of Language Generation
## By Kathleen McKeown

## Present
Biggest achievement of neural generation models:
* NLP tools are now publicly available. Democratization of NLP has been a big achievement.
* Simplicity and effectiveness of neural MT systems is amazing.   
* In the past, generation fron n-gram LM was terrible. We can now even generate text conditioned on images/video etc.
* Attention mechanisms.
* Successful on leaderboard and very good at generating long coherent text.
* Neural networks are the only scalable way of fitting highly non-linear functions in a scalable manner.
* Representation learning.


## Past
* Interdisciplinary theories at play:
    * Linguistics: Centering theory:  Pragmatic constraints.
    * Philosophy: Speech acts; Grice

* Evaluation: Lack of evaluation data.  <br/>

* Move towards Statistical NLP from 1990-2000

## Future
* Constraint on choice in language generation.
    * Planning, purpose: Maybe expressed as latent variables.
    * Spurious correlations are a problem.
    * Non-trivial error debugging.

* Looking at the data
    * Good from human eyes not just metrics!
    * General purpose models which solve many tasks hide away the details of how the model is performing on any single task

* Learning from traditional AI and other disciplines:
    * Need more intermediate representations.
    * Lapata: We will move towards symbolic intermediate representations for multi-doc summarization and other complicated problems.
    * Causality, events, tracking world.

* Are we solving the right problems?
    * How can we work with smaller but more interesting datasets?
    * Solving a task rather than dataset remains a big challenge.


* Conclusion
    * Interdisciplinary approaches have lasting impact.
    * Analyze your output
    * Bring language back to NLP
